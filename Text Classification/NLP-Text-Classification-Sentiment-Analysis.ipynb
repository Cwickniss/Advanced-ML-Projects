{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#%matplotlib inline\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import tweepy\n",
    "import re\n",
    "\n",
    "\n",
    "import twitterconfig as tc\n",
    "#from sklearn import datasets\n",
    "\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tweepy\n",
    "#!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    return re.sub(r'http\\S+', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_url('this is to test http://localhost:8888/notebooks/ how to remove url ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To be able to download twitter data\n",
    "* Sign in to twitter and go to: https://developer.twitter.com/\n",
    "\n",
    "* After logging in to the portal, and going to \"Applications\", a new application can be created which will provide the needed data for communicating with Twitter API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "# Variables that contains the user credentials to access Twitter API \n",
    "ACCESS_TOKEN = tc.ACCESS_TOKEN\n",
    "ACCESS_TOKEN_SECRET = tc.ACCESS_TOKEN_SECRET \n",
    "CONSUMER_KEY = tc.CONSUMER_KEY \n",
    "CONSUMER_SECRET = tc.CONSUMER_SECRET\n",
    "\n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True)\n",
    "        \n",
    "with open(\"test_twitter1.txt\", \"w\") as file:\n",
    "    # you ignore all retweets by adding -filter:retweets to your query.\n",
    "    for tweet_info in tweepy.Cursor(api.search, q='Python', lang = 'en', tweet_mode='extended').items(10):\n",
    "        #(not tweet.retweeted) and ('RT @' not in tweet.text)\n",
    "        if 'retweeted_status' in dir(tweet_info):\n",
    "            full_text = tweet_info.retweeted_status.full_text\n",
    "        else:\n",
    "            full_text = tweet_info.full_text\n",
    "\n",
    "        full_text = remove_emoji(full_text)\n",
    "        no_urls_no_tags = \" \".join([word for word in full_text.split()\n",
    "                                    if 'http' not in word\n",
    "                                        and not word.startswith('@')\n",
    "                                        and word != 'RT'\n",
    "                                    ])\n",
    "        # save the tweet to the file\n",
    "        file.write(no_urls_no_tags+\"\\n\")\n",
    "\n",
    "        print(tweet_info._json['user']['screen_name'],':',no_urls_no_tags)\n",
    "        print(no_urls_no_tags)\n",
    "        print('-----')\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_twitter1.txt') as f:\n",
    "    content = f.read()\n",
    "    \n",
    "wordcloud = WordCloud(\n",
    "                      #font_path='/Users/nsadawi/Library//R/3.2/library/rmarkdown/rmd/h/bootstrap-3.3.5/css/fonts/Roboto.ttf',\n",
    "                      stopwords=STOPWORDS,\n",
    "                      background_color='black',\n",
    "                      width=1800,\n",
    "                      height=1400\n",
    "                     ).generate(content)\n",
    "\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.savefig('my_wordcloud_1.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data\n",
    "\n",
    "We'll use Pandas's **read_csv**, to load an already existing dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'testdata.manual.2009.06.14.csv',header=None, encoding=\"cp1252\")\n",
    "#df = pd.read_csv(r'\\\\ikb\\\\home\\\\n\\\\csstnns\\\\Downloads\\\\twitter_data\\\\training.1600000.processed.noemoticon.csv',header=None, encoding=\"cp1252\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data does not have column names so let's provide them\n",
    "df.columns = ['polarity', 'tweet ID', 'date', 'query', 'username' , 'tweet']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are interested in two columns only: 'tweet' and 'polarity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['tweet','polarity']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's change column names for compatibility\n",
    "df.columns = ['Text', 'Category']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use **groupby** to use describe by label, this way we can begin to think about the features that separate different categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Category').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we continue our analysis we want to start thinking about the features we are going to be using. This goes along with the general idea of [feature engineering](https://en.wikipedia.org/wiki/Feature_engineering). The better your domain knowledge on the data, the better your ability to engineer more features from it. Feature engineering is a very large part of text classification in general. I encourage you to read up on the topic!\n",
    "\n",
    "Let's make a new column to detect how long each text entry is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length here is the number of chars\n",
    "df['length'] = df['Text'].apply(len)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text'][321]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "clf = MultinomialNB()\n",
    "scores = cross_val_score(clf, text_tfidf, df['Category'],  cv=8)\n",
    "#scores\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a RandomForest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier()\n",
    "scores = cross_val_score(clf, text_tfidf, df['Category'],  cv=8)\n",
    "#scores\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "start = time.time()\n",
    "clf = MultinomialNB()\n",
    "#classifier = Pipeline([('vectorizer', TfidfVectorizer(stop_words=stopwords.words('english') + list(string.punctuation))),('classifier', LinearSVC(C=10))])\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_tfidf, df['Category'], test_size=0.2, random_state=11)\n",
    "clf.fit(X_train, y_train)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Accuracy: \" + str(clf.score(X_test, y_test)) + \", Time duration: \" + str(end - start))\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot confusion_matrix\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "sns.heatmap(conf_mat, annot=True, cmap = \"Set3\", fmt =\"d\",\n",
    "xticklabels=df.Category.unique(), yticklabels=df.Category.unique())\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
